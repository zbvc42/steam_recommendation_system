{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ast import literal_eval\n",
    "from os import chdir, walk\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import LocalCluster\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates dask cluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up two directories, to project base directory\n",
    "chdir(\"../..\")\n",
    "\n",
    "# Instantiates empty list to hold file paths\n",
    "paths = []\n",
    "\n",
    "# Gets paths to review JSONs by appending file name to name of containing folder\n",
    "for dir, _, items in walk('data/reviews/'):\n",
    "    for item in items:\n",
    "        paths.append(dir+'/'+item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(row: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Extracts and formats Pandas DataFrame of reviews from within JSON super-DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Pandas Series corresponding to contents of one JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Pandas DataFrame containing relevant contents of reviews JSON (i.e, excluding metadata).\n",
    "    \"\"\"\n",
    "\n",
    "    # Parses review information as a dictionary (from string)\n",
    "    dict_reviews = literal_eval(row[\"reviews\"])\n",
    "\n",
    "    # Parses review information as a pandas DataFrame\n",
    "    df_reviews = pd.DataFrame(dict_reviews)\n",
    "    \n",
    "    # Adds appid to review DataFrame\n",
    "    df_reviews[\"steam_appid\"] = row[\"path\"]\n",
    "\n",
    "    # Extracts author information as separate DataFrame\n",
    "    df_author_info = df_reviews[\"author\"].apply(pd.Series)\n",
    "\n",
    "    # Joins author information onto reviews DataFrame\n",
    "    df_reviews = df_reviews.join(df_author_info.add_prefix(\"author_\"))\n",
    "\n",
    "    # Drops (now redundant) author information field\n",
    "    df_reviews = df_reviews.drop(\"author\", axis=1)\n",
    "\n",
    "    # Reformats datetime columns\n",
    "    for datetime_col in [\n",
    "        \"timestamp_created\",\n",
    "        \"timestamp_updated\",\n",
    "        \"author_last_played\",\n",
    "    ]:\n",
    "        df_reviews[datetime_col] = pd.to_datetime(df_reviews[datetime_col], unit=\"s\")\n",
    "\n",
    "\n",
    "    # Coerces columns that should be ints to ints\n",
    "    for int_col in [\n",
    "        \"recommendationid\",\n",
    "        \"author_steamid\",\n",
    "        \"steam_appid\",\n",
    "    ]:\n",
    "        df_reviews[int_col] = df_reviews[int_col].astype(int)\n",
    "\n",
    "    # Coerces column that should be float to float\n",
    "    df_reviews[\"weighted_vote_score\"] = df_reviews[\"weighted_vote_score\"].astype(float)\n",
    "\n",
    "    # Sets index to recommendation id column\n",
    "    df_reviews = df_reviews.set_index(\"recommendationid\")\n",
    "\n",
    "    return df_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_reviews(paths:list, batch_index:int, batch_count:int, meta:pd.DataFrame):\n",
    "    \"\"\"Imports reviews from a batch of review JSONs.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (list): List of paths to review JSONs. Should refer to some subset of all review JSONs.\n",
    "        batch_index (int): Index of current batch (for progress reporting).\n",
    "        batch_count (int): Total number of batches (for progress reporting).\n",
    "        meta (pd.DataFrame): Empty pandas DataFrame dask uses as a template.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reads review JSONs into a dask DataFrame\n",
    "    ddf_review_jsons = dd.read_json(paths, include_path_column=True)\n",
    "\n",
    "    # Trims values of path column to just name of containing folder.\n",
    "    # (Review download function uses steam app ids as folder names)\n",
    "    ddf_review_jsons[\"path\"] = (\n",
    "        ddf_review_jsons[\"path\"].astype(str).str[:-14].replace(\".+/\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Extracts reviews into dask DataFrame of pandas DataFrames (using template)\n",
    "    ddf_reviews_dfs = ddf_review_jsons.apply(extract_reviews, axis=1, meta=meta)\n",
    "\n",
    "    # Extracts pandas DataFrames of recommendation summaries from dask DataFrame;\n",
    "    #   Concatenates into dask DataFrame\n",
    "    ddf_reviews = dd.concat(ddf_reviews_dfs.compute().tolist())\n",
    "    clear_output()\n",
    "    print(f\"Batch {batch_index+1}/{batch_count} loaded!\")\n",
    "\n",
    "    # Repartitions to one partition so that only one parquet is written per batch\n",
    "    ddf_reviews = ddf_reviews.repartition(npartitions=1)\n",
    "\n",
    "    # Writes reviews to parquet file.\n",
    "    #   Append is false on first loop, so it should overwrite any parquet files from previous runs.\n",
    "    ddf_reviews.to_parquet(\"data/reviews.parquet\", append=(batch_index > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates some variables for batch processing step\n",
    "\n",
    "# Imports template DataFrame so dask knows to to interpret things\n",
    "meta = pd.read_parquet(\"dask_templates/reviews_meta.parquet\")\n",
    "\n",
    "# Sets batch size. 2^13 works pretty well with 64 GB of RAM;\n",
    "#   You'll probably want to experiment if you have less.\n",
    "#   I'd (naively) recommend 2^12 for 32 GB RAM, 2^11 for 16 GB RAM, etc.\n",
    "batch_size = 2**13\n",
    "\n",
    "# Calculates batch count for better progress reporting.\n",
    "batch_count = len(paths) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "for batch_index, paths_subset in enumerate(np.array_split(paths, len(paths)/batch_size)):\n",
    "    ingest_reviews(paths_subset.tolist(), batch_index, batch_count, meta)\n",
    "\n",
    "# Progress reporting\n",
    "clear_output()\n",
    "print('Ingest finished!')\n",
    "\n",
    "# Shuts down dask cluster\n",
    "client.shutdown()\n",
    "print(\"Dask cluster terminated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
